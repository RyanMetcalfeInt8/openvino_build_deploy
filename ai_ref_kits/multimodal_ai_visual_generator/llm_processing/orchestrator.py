import openvino_genai as ov_genai

LLM_SYSTEM_MESSAGE_START="""
You are a specialized helper bot designed to process live transcripts from a demo called "AI Adventure Game", which showcases a tabletop adventure game with live illustrations generated by a text-to-image model.

Your role is to act as a filter:

Detect descriptions of game scenes from the transcript that require illustration.
Output a detailed SD Prompt for these scenes.
When you detect a scene for the game, output it as:

SD Prompt: <a detailed prompt for illustration>

Guidelines:
Focus only on game scenes: Ignore meta-comments, explanations about the demo, or incomplete thoughts.
Contextual Awareness: Maintain and apply story context, such as the location, atmosphere, and objects, when crafting prompts. Update this context only when a new scene is explicitly described.
No Players in Prompts: Do not include references to "the player," "the players,"  "the party", or any specific characters in the SD Prompt. Focus solely on the environment and atmosphere.
Prioritize Clarity: If unsure whether the presenter is describing a scene, return: 'None'. Avoid making assumptions about incomplete descriptions.
Enhance Visuals: Add vivid and descriptive details to SD Prompts, such as lighting, mood, style, or texture, when appropriate, but stay faithful to the transcript.
Examples:
Example 1:
Input: "Let me explain how we are using AI for these illustrations." Output: 'None'

Example 2:
Input: "The party is standing at the gates of a large castle." Output: SD Prompt: "A massive medieval castle gate with towering stone walls, surrounded by mist and faintly glowing lanterns at dusk."

Example 3:
Context: "The party is at the gates of a large castle." Input: "The party then encounters a huge dragon." Output: SD Prompt: "A massive dragon with gleaming scales, standing before the misty gates of a towering medieval castle, lit by glowing lanterns under a dim sky."

Example 4:
Input: "And now the players roll for initiative." Output: 'None'

The presenter of the demo is aware of your presence and role, and will sometimes refer to you as the 'LLM', the 'agent', etc. Occasionally he will point out your roles and read back the SD prompts that you generate. When you detect this, return 'None'.

The SD prompts should be no longer than 25 words.

Only output SD prompts it is detected that there is big difference in location as compared with the last SD prompt that you gave.

Example 1:

Input 0: "The party is standing at the gates of a large castle." Output 0: SD Prompt: "A massive medieval castle gate with towering stone walls, surrounded by mist and faintly glowing lanterns at dusk."
Input 1: "A character is still at the gates of the castle." Output 1: 'None'

"""

LLM_SYSTEM_MESSAGE_END="""

Additional hints and reminders:
* You are a filter, not a chatbot. Only provide SD Prompts or 'None.'
* No Extra Notes: Do not include explanations, comments, or any text beyond the required SD Prompt or 'None.'
* Validate Completeness: A description of a scene often involves locations, objects, or atmosphere and is unlikely to be inferred from just verbs or generic phrases.
* If it seems that the transcription of the presenter is simply reading a previous SD prompt that you generated, return 'None'
* The SD prompts should be no longer than 25 words.
* Do not provide SD prompts for what seem like incomplete thoughts. Return 'None' in this case.
* Use the given theme of the game to help you decide whether or not the given bits of transcript are describing a new scene, or not.
* Do not try to actually illustrate the characters themselves, only details of their environmental surroundings & atmosphere.
* The SD prompts should be no longer than 25 words.
* Only output SD prompts it is detected that there is big difference in location as compared with the last SD prompt that you gave. If it seems like the location is the same, just return 'None'
"""

class LLMOrchestrator:
    def __init__(self, llm_pipeline, ui_update_queue, theme):
        self.llm_pipeline =  llm_pipeline
        self.ui_update_queue = ui_update_queue
        
        generate_config = ov_genai.GenerationConfig()
        generate_config.temperature = 0.7
        generate_config.top_p = 0.95
        generate_config.max_length = 2048
        generate_config.apply_chat_template = False  
        
        self.generate_config = generate_config
        
        self.meaningful_message_pairs = []
        self.system_message = self.create_system_message(theme)
        
    def create_system_message(self, theme):
        system_message=LLM_SYSTEM_MESSAGE_START
        system_message+="\nThe presenter is giving a hint that the theme of their game is: " + theme
        system_message+="\nYou should use this hint to guide your decision about whether the presenter is describing a scene from the game, or not, and also to generate adequate SD Prompts."
        system_message+="\n" + LLM_SYSTEM_MESSAGE_END
        return system_message
        
    def llm_streamer(self, subword):
        print(subword, end='', flush=True)
        self.stream_message += subword

        search_string = "SD Prompt:"
        if search_string in self.stream_message and 'None' not in self.stream_message:
            if self.stream_sd_prompt_index is None:
                self.stream_sd_prompt_index = self.stream_message.find(search_string)

            start_index = self.stream_sd_prompt_index
            # Calculate the start index of the new string (1 character past the ':')
            prompt = self.stream_message[start_index + len(search_string):].strip()

            self.ui_update_queue.put(("caption", prompt,))
            #self.caption_updated.emit(prompt)
            
        elif 'None' in self.stream_message:
            #Sometimes the LLM gives a response like: None (And then some long description why in parenthesis)
            # Basically, as soon as we see 'None', just stop generating tokens.
            return True

        # Return flag corresponds whether generation should be stopped.
        # False means continue generation.
        return False
        
    def run(self, transcription):
        print("transcription = ", transcription)
        llm_tokenizer = self.llm_pipeline.get_tokenizer()
        
        chat_history = [{"role": "system", "content": self.system_message}]
        
        #only keep the latest 2 meaningful message pairs (last 2 illustrations)
        self.meaningful_message_pairs = self.meaningful_message_pairs[-2:]
        
        for meaningful_pair in self.meaningful_message_pairs:
            user_message = meaningful_pair[0]
            assistant_response = meaningful_pair[1]

            chat_history.append({"role": "user", "content": user_message["content"]})
            chat_history.append({"role": "assistant", "content": assistant_response["content"]})
            
        chat_history.append({"role": "user", "content": transcription})
        formatted_prompt = llm_tokenizer.apply_chat_template(history=chat_history, add_generation_prompt=True)
        
        self.stream_message = ""
        self.stream_sd_prompt_index = None
        
        print("***********************************************************************************")
        print(formatted_prompt)
        print("***********************************************************************************")
        llm_result = self.llm_pipeline.generate(inputs=formatted_prompt, generation_config=self.generate_config, streamer=self.llm_streamer)
        
        #print(llm_result)
        search_string = "SD Prompt:"

        #sometimes the llm will return 'SD Prompt: None', so filter out that case.
        if search_string in llm_result and 'None' not in llm_result:
            # Find the start of the search string
            start_index = llm_result.find(search_string)
            # Calculate the start index of the new string (1 character past the ':')
            prompt = llm_result[start_index + len(search_string):].strip()
            #print(f"Extracted prompt: '{prompt}'")

            # this was a meaningful message!
            self.meaningful_message_pairs.append(
            ({"role": "user", "content": transcription},
             {"role": "assistant", "content": llm_result},)
            )
            
            return prompt
        else:
            return None
