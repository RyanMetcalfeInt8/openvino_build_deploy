from async_worker import AsyncWorker
from queue import Empty   
from multiprocessing import Queue

LLM_SYSTEM_MESSAGE_START="""
You are a specialized helper bot designed to process live transcripts from a demo called "AI Adventure Game", which showcases a tabletop adventure game with live illustrations generated by a text-to-image model.

Your role is to act as a filter:

Detect descriptions of game scenes from the transcript that require illustration.
Output a detailed SD Prompt for these scenes.
When you detect a scene for the game, output it as:

SD Prompt: <a detailed prompt for illustration>

Guidelines:
Focus only on game scenes: Ignore meta-comments, explanations about the demo, or incomplete thoughts.
Contextual Awareness: Maintain and apply story context, such as the location, atmosphere, and objects, when crafting prompts. Update this context only when a new scene is explicitly described.
No Players in Prompts: Do not include references to "the player," "the players,"  "the party", or any specific characters in the SD Prompt. Focus solely on the environment and atmosphere.
Prioritize Clarity: If unsure whether the presenter is describing a scene, return: 'None'. Avoid making assumptions about incomplete descriptions.
Enhance Visuals: Add vivid and descriptive details to SD Prompts, such as lighting, mood, style, or texture, when appropriate, but stay faithful to the transcript.
Examples:
Example 1:
Input: "Let me explain how we are using AI for these illustrations." Output: 'None'

Example 2:
Input: "The party is standing at the gates of a large castle." Output: SD Prompt: "A massive medieval castle gate with towering stone walls, surrounded by mist and faintly glowing lanterns at dusk."

Example 3:
Context: "The party is at the gates of a large castle." Input: "The party then encounters a huge dragon." Output: SD Prompt: "A massive dragon with gleaming scales, standing before the misty gates of a towering medieval castle, lit by glowing lanterns under a dim sky."

Example 4:
Input: "And now the players roll for initiative." Output: 'None'

The presenter of the demo is aware of your presence and role, and will sometimes refer to you as the 'LLM', the 'agent', etc. Occasionally he will point out your roles and read back the SD prompts that you generate. When you detect this, return 'None'.

The SD prompts should be no longer than 25 words.

Only output SD prompts it is detected that there is big difference in location as compared with the last SD prompt that you gave.

Example 1:

Input 0: "The party is standing at the gates of a large castle." Output 0: SD Prompt: "A massive medieval castle gate with towering stone walls, surrounded by mist and faintly glowing lanterns at dusk."
Input 1: "A character is still at the gates of the castle." Output 1: 'None'

"""

LLM_SYSTEM_MESSAGE_END="""

Additional hints and reminders:
* You are a filter, not a chatbot. Only provide SD Prompts or 'None.'
* No Extra Notes: Do not include explanations, comments, or any text beyond the required SD Prompt or 'None.'
* Validate Completeness: A description of a scene often involves locations, objects, or atmosphere and is unlikely to be inferred from just verbs or generic phrases.
* If it seems that the transcription of the presenter is simply reading a previous SD prompt that you generated, return 'None'
* The SD prompts should be no longer than 25 words.
* Do not provide SD prompts for what seem like incomplete thoughts. Return 'None' in this case.
* Use the given theme of the game to help you decide whether or not the given bits of transcript are describing a new scene, or not.
* Do not try to actually illustrate the characters themselves, only details of their environmental surroundings & atmosphere.
* The SD prompts should be no longer than 25 words.
* Only output SD prompts it is detected that there is big difference in location as compared with the last SD prompt that you gave. If it seems like the location is the same, just return 'None'
"""


class LLMWorker(AsyncWorker):
    def __init__(self, transcription_queue, sd_prompt_queue, ui_update_queue, llm_device, theme):
        super().__init__()
        
        self.transcription_queue = transcription_queue
        self.sd_prompt_queue = sd_prompt_queue
        self.llm_device = llm_device
        self.theme = theme
        self.ui_update_queue = ui_update_queue
        
    def _work_loop(self):
        import openvino_genai as ov_genai
        
        print("Creating an llm pipeline to run on ", self.llm_device)
        
        llm_model_path = r"./models/llama-3.1-8b-instruct/INT4_compressed_weights"
        llm_device = self.llm_device
        
        if llm_device == 'NPU':
            pipeline_config = {"MAX_PROMPT_LEN": 1536}
            llm_pipe = ov_genai.LLMPipeline(llm_model_path, llm_device, pipeline_config)
        else:
            llm_pipe = ov_genai.LLMPipeline(llm_model_path, llm_device)
            
        print("done creating llm")
        
        llm_tokenizer = llm_pipe.get_tokenizer()
        
        # Assemble the system message.
        system_message=LLM_SYSTEM_MESSAGE_START
        system_message+="\nThe presenter is giving a hint that the theme of their game is: " + self.theme
        system_message+="\nYou should use this hint to guide your decision about whether the presenter is describing a scene from the game, or not, and also to generate adequate SD Prompts."
        system_message+="\n" + LLM_SYSTEM_MESSAGE_END
        
        
        
        generate_config = ov_genai.GenerationConfig()
        generate_config.temperature = 0.7
        generate_config.top_p = 0.95
        generate_config.max_length = 2048
        generate_config.apply_chat_template = False
        
        #print("generate_config.apply_chat_template = ", generate_config.apply_chat_template)
        
        stream_message = ""
        stream_sd_prompt_index = None
        ui_update_queue = self.ui_update_queue
        
        meaningful_message_pairs = []
        
        def llm_streamer(subword):
            nonlocal stream_message
            nonlocal stream_sd_prompt_index
            nonlocal ui_update_queue
            print(subword, end='', flush=True)
            stream_message += subword

            search_string = "SD Prompt:"
            if search_string in stream_message and 'None' not in stream_message:
                if stream_sd_prompt_index is None:
                    stream_sd_prompt_index = stream_message.find(search_string)

                start_index = stream_sd_prompt_index
                # Calculate the start index of the new string (1 character past the ':')
                prompt = stream_message[start_index + len(search_string):].strip()

                ui_update_queue.put(("caption", prompt,))
                #self.caption_updated.emit(prompt)
                
            elif 'None' in stream_message:
                #Sometimes the LLM gives a response like: None (And then some long description why in parenthesis)
                # Basically, as soon as we see 'None', just stop generating tokens.
                return True

            # Return flag corresponds whether generation should be stopped.
            # False means continue generation.
            return False

        while self._running.value:
            try:
                #self.progress_updated.emit(0, "listening")
                
                # Each queue entry is a tuple:
                # (start_time_in_seconds, segment_length_in_seconds, transcription)
                queue_entry = self.transcription_queue.get(timeout=1)
                result = queue_entry[2]
                result = f"\"{result}\""
                
                #self.progress_updated.emit(0, "processing")
                
                chat_history = [{"role": "system", "content": system_message}]
                
                #only keep the latest 2 meaningful message pairs (last 2 illustrations)
                meaningful_message_pairs = meaningful_message_pairs[-2:]
                
                formatted_prompt = system_message
                
                for meaningful_pair in meaningful_message_pairs:
                    user_message = meaningful_pair[0]
                    assistant_response = meaningful_pair[1]

                    chat_history.append({"role": "user", "content": user_message["content"]})
                    chat_history.append({"role": "assistant", "content": assistant_response["content"]})
                    
                chat_history.append({"role": "user", "content": result})
                formatted_prompt = llm_tokenizer.apply_chat_template(history=chat_history, add_generation_prompt=True)
                
                #self.progress_updated.emit(0, "processing...")
                stream_message=""
                stream_sd_prompt_index=None
                
                print("***************************************************************************************")
                print(formatted_prompt)
                print("***************************************************************************************")
                llm_result = llm_pipe.generate(inputs=formatted_prompt, generation_config=generate_config, streamer=llm_streamer)
                
                #print(llm_result)
                search_string = "SD Prompt:"

                #sometimes the llm will return 'SD Prompt: None', so filter out that case.
                if search_string in llm_result and 'None' not in llm_result:
                    # Find the start of the search string
                    start_index = llm_result.find(search_string)
                    # Calculate the start index of the new string (1 character past the ':')
                    prompt = llm_result[start_index + len(search_string):].strip()
                    #print(f"Extracted prompt: '{prompt}'")

                    caption = prompt
                    #self.caption_updated.emit(caption)
                    #self.progress_updated.emit(0, "illustrating...")
                    self.sd_prompt_queue.put(prompt)

                    # this was a meaningful message!
                    meaningful_message_pairs.append(
                    ({"role": "user", "content": result},
                     {"role": "assistant", "content": llm_result},)
                    )
      
            except Empty:
                continue  # Queue is empty, just wait
                
def test_main():
    sd_prompt_queue = Queue()
    transcription_queue = Queue()
    ui_update_queue = Queue()
    
    llm_worker = LLMWorker(transcription_queue, sd_prompt_queue, ui_update_queue, "GPU", "Medieval Fantasy Adventure")
    llm_worker.start()

    try:
        while True:
            try:
                transcription = input('')
            except EOFError:
                break

            transcription_queue.put(transcription)
            
    except KeyboardInterrupt:
        print("Main: Stopping workers...")
        llm_worker.stop()
        
if __name__ == "__main__":
    test_main()